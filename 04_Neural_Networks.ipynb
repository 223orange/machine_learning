{
 "metadata": {
  "name": "04_Neural_Networks"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h1>Neural Networks </h1>\n",
      "\n",
      "Neural networks are one approach to machine learning that attempts to deal with the problem of large data dimensionality. The neural network approach uses a fixed number of basis functions - in contrast\n",
      "to methods such as support vector machines that attempt to adapt the number of basis functions - that are themselves parameterized by the model parameters. This is a significant departure from \n",
      "linear regression and logistic regression methods where the models consisted of linear combinations of fixed basis functions, $\\phi(\\mathbf{x})$, that dependend only on the input vector, $\\mathbf{x}$. In neural networks, the basis functions can now depend on both the model parameters and the input vector and thus take the form $\\phi(\\mathbf{x} | \\mathbf{w})$. \n",
      "\n",
      "Here we will cover only **feed-forward** neural networks. One can envision a neural network as a series of layers where each layer has some number of nodes and each node is a function. Each layer represents a single linear regression model. The nodes are connected to each other through inputs accepted from and outputs passed to other nodes. A *feed-forward* network is one in which these connections do form any directed cycles. See [here](http://en.wikipedia.org/wiki/Feedforward_neural_network) for more detail.\n",
      "\n",
      "![alt text](files/images/Feed_forward_neural_net.gif \"A feed forward network.\")\n",
      "\n",
      "As a matter of convention, we will refer the model as a $N$ layer model where $N$ is the number of layers for which adaptive parameters, $\\mathbf{w}$, must be determined. Thus for a model consisting of an input layer, one hidden layer, and an output layer, we consider $N$ to be 2 since parameters are determined only for the hidden and output layers."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h2>Feed-forward Network Functions</h2>\n",
      "We will consider a basic two layer nueral network model, i.e a model that maps inputs to a hidden layer and then to an output layer. We will make **the following assumptions**\n",
      "\n",
      "1. The final output will be a vector $Y$ with $K$ elements, $y_k$, where $y_k(\\mathbf{x},\\mathbf{w}) = p(C_1|\\mathbf{x})$ is the probability that node $k$ is in class $C_1$ and $p(C_2|\\mathbf{x}) = 1-p(C_1|\\mathbf{x})$\n",
      "2. The activation function at the final layer is the logistic sigmoid, $\\sigma(a) = 1 / (1 +\\exp(-a))$\n",
      "3. The activation function at the hidden layer, $h$, is an arbitrary nonlinear function of a linear combination of the parameters, $\\mathbf{w}$, and the input, $\\mathbf{x}$\n",
      "4. The network is fully connected, i.e. every node at the input layer is connected to every node in the hidden layer and every node in the hidden layer is connected to every node in the output layer\n",
      "5. A bias parameter is included at the hidden and output layers\n",
      "\n",
      "Working from the input layer toward the output layer, we can build this model as follows:\n",
      "\n",
      "<h3>Input Layer</h3>\n",
      "Assume we have an input vector $\\mathbf{x} \\in \\Re^D$. Then the input layer consists of $D+1$ nodes where the value of the $i^{th}$ node for $i=0\\ldots D$, is 0 if $i=0$ and $x_i$, i.e. the $i^{th}$ value of $\\mathbf{x}$, otherwise.\n",
      "\n",
      "<h3>Hidden Layer</h3>\n",
      "At the hidden layer we construct $M$ nodes where the value of $M$ depends on the specifics of the particular modelling problem. For each node, we define a *unit activation*, $a_m$, for $m=1\\ldots M$ as <br/>\n",
      "$a_m = \\sum_{i=0}^D w_{ji}^{(1)}x_i$ <br/>\n",
      "where the $(1)$ superscript indicates this weight is for the hidden layer. The output from each node, $z_m$, is then given by the value of a *fixed nonlinear function*, $h$, known as the *activation function*, acting on the unit activation<br/>\n",
      "$z_m = h(a_m) = h \\left( \\sum_{i=0}^D w_{mi}^{(1)}x_i \\right)$<br/>\n",
      "Notice that $h$ is the same function for all nodes.\n",
      "<h3>Output Layer</h3>\n",
      "The process at the output layer is essentially the same as at the hidden layer. We construct $K$ nodes, where again the value of $K$ depends on the specific modeling problem. For each node, we again define a *unit activation*, $a_k$, for $k=1 \\ldots K$ by<br/>\n",
      "$a_k = \\sum_{m=0}^M w_{km}^{(2)} z_m$ <br/>\n",
      "We again apply a nonlinear activation function to produce the output. Rather than being an arbitrary function, we shall assume that the activation function is the logistic sigmoid, so that <br/>\n",
      "$y_k = \\sigma(a_k)$\n",
      "\n",
      "Thus, the entire model can be summarized as a $K$ dimensional output vector $Y \\in \\Re^K$ where each element $y_k$ is in the range $[0,1)$ and given by<br/>\n",
      "$y_k(\\mathbf{x},\\mathbf{w}) = \\sigma \\left( \\sum_{m=0}^M w_{km}^{(2)} h \\left( \\sum_{i=0}^D w_{mi}^{(1)}x_i \\right) \\right)$\n",
      "\n",
      "<h3>Generalizations</h3>\n",
      "There are a wide variety of generalizations possible for this model. Some of the more important ones for practical applications include\n",
      "\n",
      "* Addition of hidden layers\n",
      "* Inclusion of *skip-layer* connections, e.g. a connection from an input node directly to an output node\n",
      "* Sparse network, i.e. not a fully connected network"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h2>Network Training</h2>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}