{
 "metadata": {
  "name": "05_Decision_Trees"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h1>Mitchell Lecture 1</h1>\n",
      "<h2>Function Approximation</h2>\n",
      "Problem Setting:\n",
      "\n",
      "*Set of possible instances $X$\n",
      "\n",
      "*Unknown target function $f:X->Y$\n",
      "\n",
      "*Set of function hypothesis $H=\\\\{h|h:X->Y\\\\}$\n",
      "\n",
      "Output:\n",
      "Hypothesis $h\\inH$ that best approximates target function $f$\n",
      "\n",
      "<h2>Decision Trees</h2>\n",
      "Will apply decision tree learning to the function approximation problem\n",
      "\n",
      "Each node in the tree tests one attribute\n",
      "Each branch from a node selects one value for the given attribute\n",
      "Each leaf node: predicts an output Y\n",
      "\n",
      "Problem Setting:\n",
      "\n",
      "*Set of possible instances X\n",
      "    - each instance $x\\inX$ is a feature vector\n",
      "    \n",
      "* Unknown target function $f:X->Y$ where Y is **discrete valued**\n",
      "\n",
      "*Set of function hypothesis $H=\\\\{h|h:X->Y}$ where each $Y$ is a decision tree\n",
      "\n",
      "Decision trees can represent **any** boolean function. It will have $2^N$ leaf nodes where $N$ is the number of boolean variables. However, other decision trees (i.e. for non-Boolean variables) the number of leaves will be different and variables may be repeated as node variables\n",
      "\n",
      "Top Down Induction of Decision Trees\n",
      "node = Root\n",
      "Main loop:\n",
      "1. A -- the \"best\" decision attribute for next node - the one variable that best classifies the data\n",
      "2. Assign A as decision attribute for node\n",
      "3. For each value of A, create new descendant of node\n",
      "4. Sort training examples to leaf nodes\n",
      "5. If training examples perfectly classified, then STOP, else iterate over new leaf nodes\n",
      "\n",
      "The question is what does \"best\" mean in step 1. The standard approach is to use the choice that maximizes information gain. First consider the entropy function:\n",
      "\n",
      "$H(X) = - \\sum_{i=1}^n P(X=i)\\log_2 P(X=i)$\n",
      "\n",
      "where $n$ is the number of possible values for $X$, specifically we minimize the conditional entropy \n",
      "\n",
      "$H(X|Y) = - \\sum_{v\\in Y}^n P(Y=v)H(X|Y=v)$\n",
      "\n",
      "Mututal information (Information Gain) of X and Y:\n",
      "\n",
      "$I(X,Y) = H(x) - H(X|Y) = H(Y) - H(Y|X)\n",
      "\n",
      "this is the expected reduction in entropy of Y"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}